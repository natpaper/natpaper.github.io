<title>1D GAN demo</title>
<div style="display: flex;justify-content:center; align-items:Center;">
    <video muted src="natgan3.mp4" type="video/mp4" controls="" autoplay="autoplay" loop="loop" width="1000px" height="600px">
    </video>
</div>

<b>Please use Chrome to display video.</b><br>

Code of 1D GAN is based on <a href=https://github.com/AYLIEN/gan-intro>gan-intro</a>
<br>
The two negative distributions are:
<pre>
class NegDistribution(object):
    def __init__(self):
        self.mu = 13
        self.sigma = 1

    def sample(self, N):
        samples = np.random.normal(self.mu, self.sigma, N)
        samples.sort()
        return samples

class NegDistribution2(object):
    def __init__(self):
        self.mu = -6
        self.sigma = 2

    def sample(self, N):
        samples = np.random.normal(self.mu, self.sigma, N)
        samples.sort()
        return samples
</pre>
<br><br>

Our supervised classification code on CIFAR 10 is based on <a href=https://github.com/kuangliu/pytorch-cifar>pytorch-cifar</a>. <br>
We implement the criterion:
<pre>
class NeutralCE(nn.Module):
    def __init__(self):
        super(NeutralCE, self).__init__()
        self.ce = nn.CrossEntropyLoss()
        self.logsoftmax = nn.LogSoftmax(dim=1)
        self.kl = nn.KLDivLoss(reduction='batchmean')
        
    def forward(self, input, target, phase='test'):
        if phase != 'train':
            return self.ce(input, target)
        pos_idx = target != -1
        neg_idx = target == -1
        ce_loss = self.ce(input[pos_idx], target[pos_idx])
        neg_loss = self.kl(self.logsoftmax(input[neg_idx]), torch.tensor([[0.1] * 10]).repeat(input[neg_idx].shape[0], 1).to(device))
        return ce_loss * sum(pos_idx) / input.shape[0] + neg_loss * sum(neg_idx) / input.shape[0]
</pre>
<br>
Note, if you use BCE loss in pytorch, the network performance on negative samples is slightly weaker compared with our results in paper.
<br><br>

For NAT-GAN on CIFAR 10, code will be available soon, and you can easily implement it by modifying
<a href=https://github.com/ZhimingZhou/AM-GAN2>AM GAN</a>, some important codes are:<br>

<pre>
fake_logits = discriminator(fake_datas, num_logits)
fake_logits2 = discriminator(tf.concat([fake_datas, next(neg_gen)[0]], 0), num_logits)

dis_fake_loss = kl_divergence(tf.ones_like(fake_logits, tf.float32) / num_logits, tf.nn.softmax(fake_logits))
dis_fake_loss2 = kl_divergence(tf.ones_like(fake_logits2, tf.float32) / num_logits, tf.nn.softmax(fake_logits2))
</pre>
set class number=10, batchsize=256, and you'd better add negative samples every 10 batches to allow model converge better.
